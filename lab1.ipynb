{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:21:57.746763Z",
     "start_time": "2020-05-21T01:21:56.474818Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab1 \n",
    "Create a bag of word model for a text classification problem. Note that this is not the same as the continous bag of word problem that we solved here but you can reuse the tokenization part.\n",
    "\n",
    "https://github.com/yanneta/ML-notebooks/blob/master/cbow.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:21:57.756878Z",
     "start_time": "2020-05-21T01:21:57.750155Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    ! wget http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
    "    ! mkdir data\n",
    "    ! tar -xvf rotten_imdb.tar.gz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:21:59.613677Z",
     "start_time": "2020-05-21T01:21:57.759507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to set locale category LC_NUMERIC to en_CN.\n",
      "Warning: Failed to set locale category LC_TIME to en_CN.\n",
      "Warning: Failed to set locale category LC_COLLATE to en_CN.\n",
      "Warning: Failed to set locale category LC_MONETARY to en_CN.\n",
      "Warning: Failed to set locale category LC_MESSAGES to en_CN.\n",
      "--2020-05-20 18:21:57--  http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
      "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.20\n",
      "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.20|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 519599 (507K) [application/x-gzip]\n",
      "Saving to: ‘rotten_imdb.tar.gz.2’\n",
      "\n",
      "rotten_imdb.tar.gz. 100%[===================>] 507.42K   452KB/s    in 1.1s    \n",
      "\n",
      "2020-05-20 18:21:59 (452 KB/s) - ‘rotten_imdb.tar.gz.2’ saved [519599/519599]\n",
      "\n",
      "mkdir: data: File exists\n",
      "x quote.tok.gt9.5000\n",
      "x plot.tok.gt9.5000\n",
      "x subjdata.README.1.0\n",
      "plot.tok.gt9.5000   quote.tok.gt9.5000  subjdata.README.1.0\n"
     ]
    }
   ],
   "source": [
    "get_data()\n",
    "! ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:21:59.626988Z",
     "start_time": "2020-05-21T01:21:59.618920Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    \"\"\" Read file returns a list of lines.\n",
    "    \"\"\"\n",
    "    with open(path, encoding = \"ISO-8859-1\") as f:\n",
    "        content = f.readlines()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.415157Z",
     "start_time": "2020-05-21T01:21:59.629544Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.472298Z",
     "start_time": "2020-05-21T01:22:00.417282Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_content = read_file(\"data/quote.tok.gt9.5000\")\n",
    "obj_content = read_file(\"data/plot.tok.gt9.5000\")\n",
    "sub_content = np.array([line.strip().lower() for line in sub_content])\n",
    "obj_content = np.array([line.strip().lower() for line in obj_content])\n",
    "sub_y = np.zeros(len(sub_content))\n",
    "obj_y = np.ones(len(obj_content))\n",
    "X = np.append(sub_content, obj_content)\n",
    "y = np.append(sub_y, obj_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.504274Z",
     "start_time": "2020-05-21T01:22:00.475075Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.518341Z",
     "start_time": "2020-05-21T01:22:00.506712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000,), (8000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.526461Z",
     "start_time": "2020-05-21T01:22:00.520875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"both lead performances are oscar-size . quaid is utterly fearless as the tortured husband living a painful lie , and moore wonderfully underplays the long-suffering heroine with an unflappable '50s dignity somewhere between jane wyman and june cleaver .\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute a vocabulary\n",
    "* Split your sentences in tokens by spliting on spaces.\n",
    "* Compute the frequency of every word.\n",
    "* Pick top frequency words (4000 or so) to be part of your vocabulary.\n",
    "* Create a map from each word to an index. Keep 0 for out of the vocabulary workds (<UNK>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.533181Z",
     "start_time": "2020-05-21T01:22:00.529141Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.539770Z",
     "start_time": "2020-05-21T01:22:00.535319Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the sentences and count the frequency of every word\n",
    "def get_vocab(content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(float)\n",
    "    for line in content:\n",
    "        words = set(line.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.664150Z",
     "start_time": "2020-05-21T01:22:00.542004Z"
    }
   },
   "outputs": [],
   "source": [
    "word_count = get_vocab(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.671777Z",
     "start_time": "2020-05-21T01:22:00.666193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21242"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.683878Z",
     "start_time": "2020-05-21T01:22:00.673806Z"
    }
   },
   "outputs": [],
   "source": [
    "# pick top frequency word by dropping words appears less than 5 times\n",
    "for word in list(word_count):\n",
    "    if word_count[word] < 5:\n",
    "        del word_count[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.690740Z",
     "start_time": "2020-05-21T01:22:00.685689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4008"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.698695Z",
     "start_time": "2020-05-21T01:22:00.692558Z"
    }
   },
   "outputs": [],
   "source": [
    "## Finally we need an index for each word in the vocab\n",
    "vocab2index = {\"UNK\":0} # init with padding and unknown\n",
    "words = [\"UNK\"]\n",
    "for word in word_count:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of word representation\n",
    "\n",
    "* Given a piece of text compute the following features $x$.\n",
    "$x_i = 1$ if word with index $i$ appears in the text. Otherwise $x_i = 0$. Note that length $x$ is the size of the vocabulary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.707614Z",
     "start_time": "2020-05-21T01:22:00.700528Z"
    }
   },
   "outputs": [],
   "source": [
    "# encode the sentence to an numpy array of 0s and 1s\n",
    "def encode_sentence(s, vocab2index):\n",
    "    N=len(vocab2index)\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in s.split()])\n",
    "    for i in enc1:\n",
    "        enc[i]=1\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.715271Z",
     "start_time": "2020-05-21T01:22:00.709871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4009"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encode_sentence(x_train[0], vocab2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dataset and dataloaders\n",
    "Write a dataset for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.725175Z",
     "start_time": "2020-05-21T01:22:00.717211Z"
    }
   },
   "outputs": [],
   "source": [
    "class BOW(Dataset):\n",
    "    def __init__(self, x, y, vocab2index):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vocab2index = vocab2index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = encode_sentence(self.x[idx], self.vocab2index)\n",
    "        return x, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.740384Z",
     "start_time": "2020-05-21T01:22:00.734556Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = BOW(x_train, y_train, vocab2index)\n",
    "val_ds = BOW(x_val, y_val, vocab2index)\n",
    "train_dl = DataLoader(train_ds, batch_size=500, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Define a simpler linear model or a two layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.750228Z",
     "start_time": "2020-05-21T01:22:00.743286Z"
    }
   },
   "outputs": [],
   "source": [
    "class BOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden=50):\n",
    "        super(BOWModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(vocab_size, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and valid functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.764118Z",
     "start_time": "2020-05-21T01:22:00.757006Z"
    }
   },
   "outputs": [],
   "source": [
    "def val_metrics(model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0\n",
    "    for x, y in valid_dl:\n",
    "        y_hat = model(x.float()).squeeze(1)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y.float())\n",
    "        y_pred = y_hat > 0\n",
    "        correct += (y_pred.float() == y.float()).float().sum()\n",
    "        total += x.size(0)\n",
    "        loss_sum += loss.item()*x.size(0)\n",
    "    accuracy = correct.item()/total\n",
    "    return loss_sum/total, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:00.776008Z",
     "start_time": "2020-05-21T01:22:00.766418Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epocs(model, train_dl, valid_dl, epochs=10, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        loss_sum = 0\n",
    "        for x, y in train_dl:\n",
    "            y_hat = model(x.float()).squeeze(1)\n",
    "            loss = F.binary_cross_entropy_with_logits(y_hat, y.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total += x.size(0)\n",
    "            loss_sum += loss.item()*x.size(0)\n",
    "        val_loss, val_acc = val_metrics(model, valid_dl)\n",
    "        print(\"train loss %.3f val loss %.3f and accuracy %.3f\" % (loss_sum/total, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:08.270695Z",
     "start_time": "2020-05-21T01:22:00.778220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.516 val loss 0.316 and accuracy 0.898\n",
      "train loss 0.213 val loss 0.227 and accuracy 0.905\n",
      "train loss 0.129 val loss 0.229 and accuracy 0.904\n",
      "train loss 0.089 val loss 0.242 and accuracy 0.900\n",
      "train loss 0.064 val loss 0.264 and accuracy 0.899\n",
      "train loss 0.049 val loss 0.283 and accuracy 0.899\n",
      "train loss 0.037 val loss 0.307 and accuracy 0.900\n",
      "train loss 0.029 val loss 0.335 and accuracy 0.895\n",
      "train loss 0.023 val loss 0.356 and accuracy 0.898\n",
      "train loss 0.018 val loss 0.375 and accuracy 0.895\n",
      "train loss 0.014 val loss 0.398 and accuracy 0.895\n",
      "train loss 0.011 val loss 0.418 and accuracy 0.895\n",
      "train loss 0.009 val loss 0.428 and accuracy 0.894\n",
      "train loss 0.008 val loss 0.445 and accuracy 0.895\n",
      "train loss 0.006 val loss 0.464 and accuracy 0.893\n"
     ]
    }
   ],
   "source": [
    "model = BOWModel(vocab_size=len(vocab2index.keys()))\n",
    "train_epocs(model,train_dl, val_dl, 15, 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word importance\n",
    "To get the words that affect the most the positive label we find the words with higher weights. Similarly to get the words that affect the most the 0 label we find the words with lower weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:08.281873Z",
     "start_time": "2020-05-21T01:22:08.272724Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.1674,  0.0476,  0.2405,  ...,  0.1993, -0.2115, -0.2042],\n",
       "         [ 0.1217,  0.0367,  0.2313,  ...,  0.1845, -0.2021, -0.2154],\n",
       "         [-0.0281,  0.1071, -0.2015,  ..., -0.1542,  0.1943,  0.2144],\n",
       "         ...,\n",
       "         [-0.0297,  0.0991, -0.1881,  ..., -0.1780,  0.1667,  0.2032],\n",
       "         [-0.0354,  0.0850, -0.2251,  ..., -0.1934,  0.2254,  0.2239],\n",
       "         [-0.0237,  0.1089, -0.2301,  ..., -0.1776,  0.2014,  0.2473]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([0.0794, 0.0718, 0.0858, 0.0717, 0.0948, 0.0900, 0.0788, 0.0955, 0.0668,\n",
       "         0.0969, 0.0814, 0.0713, 0.0801, 0.0871, 0.0753, 0.1152, 0.0575, 0.0674,\n",
       "         0.0780, 0.0720, 0.0780, 0.0678, 0.0751, 0.0756, 0.0801, 0.0636, 0.0977,\n",
       "         0.0825, 0.0846, 0.1001, 0.1029, 0.0747, 0.0980, 0.0740, 0.0787, 0.1016,\n",
       "         0.0570, 0.0766, 0.0916, 0.1093, 0.1020, 0.0752, 0.0774, 0.0786, 0.0919,\n",
       "         0.0612, 0.2007, 0.0948, 0.0989, 0.1111], requires_grad=True), Parameter containing:\n",
       " tensor([[-0.3645, -0.4358,  0.4442, -0.4066,  0.4245,  0.4636,  0.4185, -0.3456,\n",
       "          -0.3604, -0.4033,  0.4690,  0.4550, -0.3947,  0.4171,  0.4405,  0.4605,\n",
       "          -0.3771, -0.3631,  0.4720,  0.4922,  0.4829, -0.4255, -0.3930, -0.3524,\n",
       "           0.4301,  0.4701,  0.4484,  0.4241,  0.4530,  0.4218,  0.4459,  0.4555,\n",
       "           0.4107, -0.5087,  0.4375,  0.4240, -0.3993, -0.3807,  0.4273,  0.4039,\n",
       "           0.4298, -0.3866, -0.4313, -0.4394,  0.4571, -0.4347,  0.8328,  0.4655,\n",
       "           0.4744,  0.4215]], requires_grad=True), Parameter containing:\n",
       " tensor([-0.0587], requires_grad=True)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parms = [p for p in model.parameters()]\n",
    "parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:08.292312Z",
     "start_time": "2020-05-21T01:22:08.283987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16735557,  0.04764399,  0.24045864, ...,  0.19933857,\n",
       "        -0.21148476, -0.20415619],\n",
       "       [ 0.12170083,  0.03665189,  0.23129152, ...,  0.18451473,\n",
       "        -0.20206094, -0.21537167],\n",
       "       [-0.02809834,  0.10714494, -0.20148   , ..., -0.1541967 ,\n",
       "         0.19432327,  0.21444435],\n",
       "       ...,\n",
       "       [-0.02966496,  0.0991099 , -0.18811   , ..., -0.17804377,\n",
       "         0.1667381 ,  0.20321971],\n",
       "       [-0.0354009 ,  0.08496693, -0.22514299, ..., -0.19336474,\n",
       "         0.22538666,  0.22393951],\n",
       "       [-0.02368307,  0.10891414, -0.23014365, ..., -0.17760113,\n",
       "         0.20144713,  0.24732086]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = parms[0].detach().numpy()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:08.301133Z",
     "start_time": "2020-05-21T01:22:08.295549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4009,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:08.306413Z",
     "start_time": "2020-05-21T01:22:08.303322Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_indeces = np.argsort(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:08.312911Z",
     "start_time": "2020-05-21T01:22:08.308204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.33721027, 0.41792864)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0, sorted_indeces[0]], weights[0, sorted_indeces[-1]],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:08.319795Z",
     "start_time": "2020-05-21T01:22:08.314760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finds',\n",
       " 'obsessed',\n",
       " 'birth',\n",
       " 'devdas',\n",
       " 'discover',\n",
       " 'kung-fu',\n",
       " 'murders',\n",
       " 'door',\n",
       " 'army',\n",
       " 'discovers']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in sorted_indeces[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T01:22:08.330136Z",
     "start_time": "2020-05-21T01:22:08.323934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['material',\n",
       " 'worth',\n",
       " 'performances',\n",
       " 'dull',\n",
       " 'laughs',\n",
       " 'screenwriters',\n",
       " 'flick',\n",
       " 'enjoy',\n",
       " '--',\n",
       " 'solid',\n",
       " 'entertaining']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in sorted_indeces[3998:]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
